{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "\n",
    "class BestFeatureExtract:\n",
    "    def __init__(self, no_feature):\n",
    "        \"\"\"\n",
    "        Initialize the BestFeatureExtract class.\n",
    "        \n",
    "        Parameters:\n",
    "        - no_feature (int): The number of features to extract.\n",
    "        \"\"\"\n",
    "        self.no_feature = no_feature\n",
    "        self.important_indices = []\n",
    "    \n",
    "    def training(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        Train the RandomForestClassifier to identify important features.\n",
    "        \n",
    "        Parameters:\n",
    "        - x_train (ndarray): The training data.\n",
    "        - y_train (ndarray): The training labels.\n",
    "        \"\"\"\n",
    "        model = RandomForestClassifier()\n",
    "        model.fit(x_train, y_train)\n",
    "        feature_importances = model.feature_importances_\n",
    "        self.important_indices = np.argsort(feature_importances)[-self.no_feature:]\n",
    "    \n",
    "    def best_feature_extract(self, x_data):\n",
    "        \"\"\"\n",
    "        Extract the best features from the dataset using the important indices.\n",
    "        \n",
    "        Parameters:\n",
    "        - x_data (ndarray): The dataset from which features are extracted.\n",
    "        \n",
    "        Returns:\n",
    "        - ndarray: Dataset reduced to the selected features.\n",
    "        \"\"\"\n",
    "        if not self.important_indices:\n",
    "            raise ValueError(\"Important indices are not initialized. Run 'training' or 'load_indices' first.\")\n",
    "        return x_data[:, self.important_indices]\n",
    "    \n",
    "    def save_indices(self, all_features):\n",
    "        \"\"\"\n",
    "        Save the important feature indices to a file.\n",
    "        \n",
    "        Parameters:\n",
    "        - all_features (int): Total number of features in the original dataset.\n",
    "        \"\"\"\n",
    "        file_name = f'best_{self.no_feature}_of_{all_features}.pkl'\n",
    "        with open(file_name, 'wb') as file:\n",
    "            pickle.dump(self.important_indices, file)\n",
    "        print(f\"Indices saved to {file_name}\")\n",
    "    \n",
    "    def load_indices(self, all_features):\n",
    "        \"\"\"\n",
    "        Load the important feature indices from a file.\n",
    "        \n",
    "        Parameters:\n",
    "        - all_features (int): Total number of features in the original dataset.\n",
    "        \"\"\"\n",
    "        file_name = f'best_{self.no_feature}_of_{all_features}.pkl'\n",
    "        try:\n",
    "            with open(file_name, 'rb') as file:\n",
    "                self.important_indices = pickle.load(file)\n",
    "            print(f\"Indices loaded from {file_name}\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"File '{file_name}' not found. Ensure the indices have been saved first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "class NLP_Converter:\n",
    "    def __init__(self, vector_size, window, mint_count, worker, sentences):\n",
    "        \n",
    "        # Initialize the Word2Vec model\n",
    "        self.model = Word2Vec(\n",
    "            sentences=sentences, \n",
    "            vector_size=vector_size, \n",
    "            window=window, \n",
    "            min_count=mint_count, \n",
    "            workers=worker\n",
    "        )\n",
    "        self.vector_size = vector_size\n",
    "\n",
    "    def get_vector(self, text):\n",
    "        vectors = [self.model.wv[word] for word in text if word in self.model.wv]\n",
    "        \n",
    "        if vectors:\n",
    "            # Compute the average vector of all token vectors\n",
    "            return sum(vectors) / len(vectors)\n",
    "        else:\n",
    "            # Return a zero vector if no tokens exist in the Word2Vec vocabulary\n",
    "            return [0] * self.vector_size\n",
    "\n",
    "    def create_nlp_extracted_feature(self, df):\n",
    "        # Apply the `get_vector` method to the 'url' column to generate vector representations\n",
    "        df[\"vector\"] = df[\"url\"].apply(self.get_vector).apply(lambda x: normalize([x])[0])\n",
    "        \n",
    "        # Create column names for individual vector components\n",
    "        vector_columns = [f'vector_{i+1}' for i in range(self.vector_size)]\n",
    "        \n",
    "        # Convert the 'vector' column into a DataFrame with individual vector components as separate columns\n",
    "        vectorized_data = pd.DataFrame(df['vector'].to_list(), columns=vector_columns)\n",
    "        \n",
    "        # Concatenate the original DataFrame with the vectorized components\n",
    "        df = pd.concat([df[['vector']], vectorized_data], axis=1)\n",
    "        \n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./dataset/v1/preprocess.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_preprocesser = NLP_Converter(1000, 5, 1, 4, data[\"preprocess_url\"])\n",
    "nlp_preprocesser = nlp_preprocesser.create_nlp_extracted_feature(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feature_extraction = BestFeatureExtract(200)\n",
    "best_feature_extraction.training(data[], data[\"label\"])\n",
    "best_feature_extraction.save_indices(1000)\n",
    "\n",
    "best_nlp_feature = self.best_feature_extraction.best_feature_extract(nlp_feature,data[\"label\"])\n",
    "data = pd.concat([data, best_nlp_feature], axis=1)\n",
    "pd.DataFrame.to_csv(\"./dataset/v1/preprocess.csv\", index=False, header=True, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
