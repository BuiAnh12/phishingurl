{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "2UdiipoMkppA"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.preprocessing import normalize\n",
        "from gensim.utils import simple_preprocess\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "class NLP_Converter:\n",
        "    def __init__(self, vector_size, window, min_count, workers, sentences):\n",
        "        # Define the model save path\n",
        "        # model_save_path = './model/word2vec_model.model'\n",
        "        # model_dir = os.path.dirname(model_save_path)\n",
        "\n",
        "        try:\n",
        "            self.model = Word2Vec.load('model\\word2vec_model.model')\n",
        "        except:\n",
        "            # # Initialize the Word2Vec model\n",
        "            # self.model = Word2Vec(\n",
        "            #     sentences=sentences,\n",
        "            #     vector_size=vector_size,\n",
        "            #     window=window,\n",
        "            #     min_count=min_count,\n",
        "            #     workers=workers\n",
        "            # )\n",
        "            # Save the model to a file\n",
        "            # self.model.save(model_save_path)\n",
        "            # print(f\"Model saved to {model_save_path}\")\n",
        "            pass\n",
        "\n",
        "        vocab = set(self.model.wv.index_to_key)\n",
        "        print(len(vocab))\n",
        "\n",
        "    def url_to_vector(self,tokens):\n",
        "        # Get vectors for each token in the URL\n",
        "        vectors = [self.model.wv[word] for word in tokens if word in self.model.wv]\n",
        "        # Average the vectors to get a single vector representation for the URL\n",
        "        if vectors:\n",
        "            return sum(vectors) / len(vectors)\n",
        "        else:\n",
        "            # Return a zero vector if no tokens exist in the Word2Vec vocabulary\n",
        "            return [0] * self.model.vector_size\n",
        "\n",
        "    def process_url(self, url):\n",
        "        \"\"\"Process a single URL and extract its NLP features.\"\"\"\n",
        "        # Tokenize using the same logic as training\n",
        "        tokens = simple_preprocess(url)\n",
        "        # print(\"Tokens in process_url:\", tokens)  # Debug\n",
        "\n",
        "        # Generate vector\n",
        "        vector = self.url_to_vector(tokens)\n",
        "\n",
        "        if not any(vector):\n",
        "            print(\"Warning: All-zero vector returned for URL\")\n",
        "\n",
        "        # Normalize vector\n",
        "        # normalized_vector = normalize([vector])[0]\n",
        "\n",
        "        # return {f'vector_{i+1}': value for i, value in enumerate(normalized_vector)}\n",
        "        return {f'vector_{i+1}': value for i, value in enumerate(vector)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "y1fCJn0Ak1eS"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "from googlesearch import search\n",
        "\n",
        "class URLProcessor:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def is_valid_url(self, url):\n",
        "        \"\"\"Check if the URL is valid.\"\"\"\n",
        "        try:\n",
        "            parsed_url = urlparse(url)\n",
        "            return parsed_url.scheme in ['http', 'https'] and bool(parsed_url.netloc)\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "    def breakdown_url(self, url):\n",
        "        try:\n",
        "            parsed_url = urlparse(url)\n",
        "            scheme = parsed_url.scheme\n",
        "            domain = parsed_url.netloc\n",
        "            path = parsed_url.path\n",
        "            query = parsed_url.query\n",
        "            fragment = parsed_url.fragment\n",
        "            path_parts = path.strip(\"/\").split(\"/\") if path else []\n",
        "            query_params = parse_qs(query)\n",
        "            query_keys = list(query_params.keys())\n",
        "            query_values = [\".\".join(values) for values in query_params.values()]\n",
        "\n",
        "            return {\n",
        "                \"original_url\": url,\n",
        "                \"scheme\": scheme,\n",
        "                \"domain\": domain,\n",
        "                \"path\": path,\n",
        "                \"path_parts\": path_parts,\n",
        "                \"query\": query,\n",
        "                \"query_keys\": query_keys,\n",
        "                \"query_values\": query_values,\n",
        "                \"fragment\": fragment,\n",
        "            }\n",
        "        except ValueError as e:\n",
        "            print(f\"Error parsing URL {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def having_ip_address(self, url):\n",
        "        if not url:\n",
        "            return 0\n",
        "        match = re.search(\n",
        "            r\"(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.\"  # IPv4\n",
        "            r\"([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.\"  # IPv4 part\n",
        "            r\"([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.\"  # IPv4 part\n",
        "            r\"([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\/)|\"  # IPv4 ending\n",
        "            r\"((?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4})\",  # IPv6\n",
        "            url,\n",
        "        )\n",
        "        return 1 if match else 0\n",
        "\n",
        "    def abnormal_url(self, url):\n",
        "        try:\n",
        "            hostname = urlparse(url).hostname or \"\"\n",
        "            return 1 if hostname in url else 0\n",
        "        except Exception:\n",
        "            return 0\n",
        "    def no_of_embed(self,url):\n",
        "        urldir = urlparse(url).path\n",
        "        return urldir.count('//')\n",
        "\n",
        "    def google_index(self, url):\n",
        "        try:\n",
        "            site = search(url, num_results=5)\n",
        "            return 1 if site else 0\n",
        "        except Exception:\n",
        "            return 0\n",
        "\n",
        "    def count_character(self, url, char):\n",
        "        if not isinstance(url, str):\n",
        "            print(f\"Invalid URL: {url}\")\n",
        "            return 0\n",
        "        # print(f\"Counting '{char}' in URL: {url}\")\n",
        "        return url.count(char)\n",
        "\n",
        "\n",
        "    def url_length(self, url):\n",
        "        return len(url)\n",
        "\n",
        "    def hostname_length(self, url):\n",
        "        return len(urlparse(url).netloc)\n",
        "\n",
        "    def suspicious_words(self, url):\n",
        "        match = re.search(\n",
        "            r\"PayPal|login|signin|bank|account|update|free|lucky|service|bonus|ebayisapi|webscr\",\n",
        "            url,\n",
        "        )\n",
        "        return 1 if match else 0\n",
        "\n",
        "    def digit_count(self, url):\n",
        "        return sum(1 for char in url if char.isdigit())\n",
        "\n",
        "    def letter_count(self, url):\n",
        "        return sum(1 for char in url if char.isalpha())\n",
        "\n",
        "    def shortening_service(self, url):\n",
        "        match = re.search(\n",
        "            r\"bit\\\\.ly|goo\\\\.gl|shorte\\\\.st|go2l\\\\.ink|x\\\\.co|ow\\\\.ly|t\\\\.co|tinyurl|tr\\\\.im|is\\\\.gd|cli\\\\.gs|\"\n",
        "            r\"yfrog\\\\.com|migre\\\\.me|ff\\\\.im|tiny\\\\.cc|url4\\\\.eu|twit\\\\.ac|su\\\\.pr|twurl\\\\.nl|snipurl\\\\.com|\"\n",
        "            r\"short\\\\.to|BudURL\\\\.com|ping\\\\.fm|post\\\\.ly|Just\\\\.as|bkite\\\\.com|snipr\\\\.com|fic\\\\.kr|loopt\\\\.us|\"\n",
        "            r\"doiop\\\\.com|short\\\\.ie|kl\\\\.am|wp\\\\.me|rubyurl\\\\.com|om\\\\.ly|to\\\\.ly|bit\\\\.do|lnkd\\\\.in|db\\\\.tt|\"\n",
        "            r\"qr\\\\.ae|adf\\\\.ly|goo\\\\.gl|bitly\\\\.com|cur\\\\.lv|tinyurl\\\\.com|ow\\\\.ly|bit\\\\.ly|ity\\\\.im|q\\\\.gs|is\\\\.gd|po\\\\.st|\"\n",
        "            r\"bc\\\\.vc|twitthis\\\\.com|u\\\\.to|j\\\\.mp|buzurl\\\\.com|cutt\\\\.us|u\\\\.bb|yourls\\\\.org|x\\\\.co|prettylinkpro\\\\.com|\"\n",
        "            r\"scrnch\\\\.me|filoops\\\\.info|vzturl\\\\.com|qr\\\\.net|1url\\\\.com|tweez\\\\.me|v\\\\.gd|tr\\\\.im|link\\\\.zip\\\\.net\",\n",
        "            url,\n",
        "        )\n",
        "        return 1 if match else 0\n",
        "\n",
        "    def process_url(self, url):\n",
        "        if not self.is_valid_url(url):\n",
        "            return {\"error\": \"Invalid URL\"}\n",
        "\n",
        "        data = self.breakdown_url(url)\n",
        "        if not data:\n",
        "            return {\"error\": \"Failed to process URL\"}\n",
        "\n",
        "        # Update features to match the specified order\n",
        "        data.update({\n",
        "            \"use_of_ip\": self.having_ip_address(url),\n",
        "            \"abnormal_url\": self.abnormal_url(url),\n",
        "            \"google_index\": self.google_index(url),\n",
        "            \"count.\": self.count_character(url, \".\"),\n",
        "            \"count-www\": self.count_character(url, \"www\"),\n",
        "            \"count@\": self.count_character(url, \"@\"),\n",
        "            \"count_dir\": self.count_character(urlparse(url).path, \"/\"),\n",
        "            \"count_embed_domian\": self.no_of_embed(url),\n",
        "            \"short_url\": self.shortening_service(url),\n",
        "            \"count%\": self.count_character(url, \"%\"),\n",
        "            \"count?\": self.count_character(url, \"?\"),\n",
        "            \"count-\": self.count_character(url, \"-\"),\n",
        "            \"count=\": self.count_character(url, \"=\"),\n",
        "            \"url_length\": self.url_length(url),\n",
        "            \"hostname_length\": self.hostname_length(url),\n",
        "            \"sus_url\": self.suspicious_words(url),\n",
        "            \"count-digits\": self.digit_count(url),\n",
        "            \"count-letters\": self.letter_count(url),\n",
        "        })\n",
        "        return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "G-cCexM4moty"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "aiHfyXcXl6Vy"
      },
      "outputs": [],
      "source": [
        "base_df = pd.read_csv(\"./dataset/v1/base_url.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZKpnnEUmtJ0",
        "outputId": "6006567e-a967-4007-a881-487bbff1744b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(450176, 2)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "pzuYiybRm0mU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load the classes\n",
            "286432\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame()\n",
        "df['preprocess_url'] = base_df['url'].apply(simple_preprocess)\n",
        "print(\"Load the classes\")\n",
        "processor = URLProcessor()\n",
        "nlp_convert = NLP_Converter(300,5,1,4,df['preprocess_url'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6cjnRx6vk25T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assume processor, nlp_convert, and detect_model are already defined\n",
        "\n",
        "def feature_extraction(url_df):\n",
        "    \"\"\"\n",
        "    Predict whether URLs in a DataFrame are phishing or not and return the final processed data.\n",
        "\n",
        "    Args:\n",
        "        url_df (pd.DataFrame): DataFrame containing a column 'url' with URLs to process.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing the final processed features for all URLs in the input DataFrame.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize lists to store features and errors\n",
        "        extracted_features_list = []\n",
        "        nlp_features_list = []\n",
        "        errors = []\n",
        "\n",
        "        for url in url_df['url']:\n",
        "            try:\n",
        "                # Preprocess the URL to extract features\n",
        "                url_features = processor.process_url(url)\n",
        "                if 'error' in url_features:\n",
        "                    errors.append({'url': url, 'error': url_features['error']})\n",
        "                    continue\n",
        "\n",
        "                # Extract NLP features\n",
        "                nlp_feature = nlp_convert.process_url(url)\n",
        "\n",
        "                # Append features for processing\n",
        "                extracted_features_list.append(url_features)\n",
        "                nlp_features_list.append(nlp_feature)\n",
        "\n",
        "            except Exception as url_exception:\n",
        "                errors.append({'url': url, 'error': str(url_exception)})\n",
        "\n",
        "        # Check if there are any valid features to process\n",
        "        if not extracted_features_list or not nlp_features_list:\n",
        "            return pd.DataFrame(errors)\n",
        "\n",
        "        # Convert extracted features and NLP features to DataFrames\n",
        "        feature_columns = [\n",
        "            'use_of_ip', 'abnormal_url', 'google_index', 'count.', 'count-www',\n",
        "            'count@', 'count_dir', 'count_embed_domian', 'short_url', 'count%',\n",
        "            'count?', 'count-', 'count=', 'url_length', 'hostname_length', 'sus_url',\n",
        "            'count-digits', 'count-letters'\n",
        "        ]\n",
        "\n",
        "        features_df = pd.DataFrame(extracted_features_list, columns=feature_columns)\n",
        "        nlp_features_df = pd.DataFrame(nlp_features_list)\n",
        "\n",
        "        # Combine the extracted features and NLP features\n",
        "        combined_df = pd.concat([features_df, nlp_features_df], axis=1)\n",
        "\n",
        "        # # Normalize the data using StandardScaler\n",
        "        # scaler = StandardScaler()\n",
        "        # scaled_data = scaler.fit_transform(combined_df)\n",
        "\n",
        "        # # Convert to NumPy arrays\n",
        "        # final_data = scaled_data.astype('float32')\n",
        "\n",
        "        # # Return the final processed DataFrame\n",
        "        # final_processed_df = pd.DataFrame(final_data, columns=combined_df.columns)\n",
        "        combined_df['url'] = url_df['url'].values\n",
        "\n",
        "        return combined_df\n",
        "\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame({'error': [str(e)]})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ba0zeDPLoB5-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(450176, 2)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_df = base_df.dropna()\n",
        "base_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "tvROBZzKoYpk"
      },
      "outputs": [],
      "source": [
        "base_df['type'] = base_df['type'].map({'phishing': 1, 'legitimate': 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "AbfrOaH4olgu"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.google.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.youtube.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.facebook.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://www.baidu.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.wikipedia.org</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         url  type\n",
              "0     https://www.google.com     0\n",
              "1    https://www.youtube.com     0\n",
              "2   https://www.facebook.com     0\n",
              "3      https://www.baidu.com     0\n",
              "4  https://www.wikipedia.org     0"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def process_in_segments(base_df, segment_count=100, output_dir=\"segments\"):\n",
        "    \"\"\"\n",
        "    Process the input DataFrame in segments, save each segment, and combine the results.\n",
        "\n",
        "    Args:\n",
        "        base_df (pd.DataFrame): The base DataFrame to process.\n",
        "        segment_count (int): The number of segments to divide the DataFrame into.\n",
        "        output_dir (str): Directory to save intermediate segment files.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Combined DataFrame of all processed segments.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
        "    segment_size = len(base_df) // segment_count\n",
        "    processed_files = []\n",
        "\n",
        "    for i in range(segment_count):\n",
        "        try:\n",
        "            start_idx = i * segment_size\n",
        "            end_idx = (i + 1) * segment_size if i < segment_count - 1 else len(base_df)\n",
        "            processed_df = pd.DataFrame()\n",
        "            segment = pd.DataFrame()\n",
        "            segment = base_df.iloc[start_idx:end_idx]\n",
        "\n",
        "            # Check the size of the segment before processing\n",
        "            print(f\"Segment {i} size before processing: {segment.shape}\")\n",
        "\n",
        "            # Skip empty segments\n",
        "            if segment.empty:\n",
        "                print(f\"Warning: Segment {i} is empty. Skipping.\")\n",
        "                continue\n",
        "            url_df = segment[['url']]\n",
        "            # type_df = segment[['type']]\n",
        "            # Apply feature extraction (ensure feature_extraction is defined elsewhere)\n",
        "            processed_df = feature_extraction(url_df)\n",
        "\n",
        "\n",
        "            # Skip empty DataFrames after cleaning\n",
        "            if processed_df.empty:\n",
        "                print(f\"Warning: Segment {i} has no valid data after cleaning. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # save_df = processed_df.join(type_df)\n",
        "            # Save the segment to a file\n",
        "            segment_file = os.path.join(output_dir, f\"segment_{i}.csv\")\n",
        "            processed_df.to_csv(segment_file, index=False)\n",
        "            processed_files.append(segment_file)\n",
        "\n",
        "            print(f\"Processed Segment {i + 1} / {segment_count}\")\n",
        "\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing segment {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Combine all segments into a single DataFrame (use try-except for this part too)\n",
        "    try:\n",
        "        combined_df = pd.concat([pd.read_csv(file) for file in processed_files], ignore_index=True)\n",
        "        combined_df = combined_df.join(base_df[\"type\"])\n",
        "        # Check the size of the combined DataFrame before cleaning\n",
        "        print(f\"Combined DataFrame size before cleaning: {combined_df.shape}\")\n",
        "\n",
        "        # Drop rows with any NaN values in the final DataFrame\n",
        "\n",
        "\n",
        "        # Check the size of the final cleaned DataFrame\n",
        "        print(f\"Combined DataFrame size after cleaning: {combined_df.shape}\")\n",
        "\n",
        "        # Save the final combined DataFrame\n",
        "        combined_df.to_csv(\"final_df_no_norm.csv\", index=False)\n",
        "        print(\"Final combined DataFrame saved.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error combining files: {e}\")\n",
        "        combined_df = None\n",
        "\n",
        "    return combined_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Segment 0 size before processing: (4501, 2)\n",
            "Processed Segment 1 / 100\n",
            "Segment 1 size before processing: (4501, 2)\n",
            "Processed Segment 2 / 100\n",
            "Segment 2 size before processing: (4501, 2)\n",
            "Processed Segment 3 / 100\n",
            "Segment 3 size before processing: (4501, 2)\n",
            "Processed Segment 4 / 100\n",
            "Segment 4 size before processing: (4501, 2)\n",
            "Processed Segment 5 / 100\n",
            "Segment 5 size before processing: (4501, 2)\n",
            "Processed Segment 6 / 100\n",
            "Segment 6 size before processing: (4501, 2)\n",
            "Processed Segment 7 / 100\n",
            "Segment 7 size before processing: (4501, 2)\n",
            "Processed Segment 8 / 100\n",
            "Segment 8 size before processing: (4501, 2)\n",
            "Processed Segment 9 / 100\n",
            "Segment 9 size before processing: (4501, 2)\n",
            "Processed Segment 10 / 100\n",
            "Segment 10 size before processing: (4501, 2)\n",
            "Processed Segment 11 / 100\n",
            "Segment 11 size before processing: (4501, 2)\n",
            "Processed Segment 12 / 100\n",
            "Segment 12 size before processing: (4501, 2)\n",
            "Processed Segment 13 / 100\n",
            "Segment 13 size before processing: (4501, 2)\n",
            "Processed Segment 14 / 100\n",
            "Segment 14 size before processing: (4501, 2)\n",
            "Processed Segment 15 / 100\n",
            "Segment 15 size before processing: (4501, 2)\n",
            "Processed Segment 16 / 100\n",
            "Segment 16 size before processing: (4501, 2)\n",
            "Processed Segment 17 / 100\n",
            "Segment 17 size before processing: (4501, 2)\n",
            "Processed Segment 18 / 100\n",
            "Segment 18 size before processing: (4501, 2)\n",
            "Processed Segment 19 / 100\n",
            "Segment 19 size before processing: (4501, 2)\n",
            "Processed Segment 20 / 100\n",
            "Segment 20 size before processing: (4501, 2)\n",
            "Processed Segment 21 / 100\n",
            "Segment 21 size before processing: (4501, 2)\n",
            "Processed Segment 22 / 100\n",
            "Segment 22 size before processing: (4501, 2)\n",
            "Processed Segment 23 / 100\n",
            "Segment 23 size before processing: (4501, 2)\n",
            "Processed Segment 24 / 100\n",
            "Segment 24 size before processing: (4501, 2)\n",
            "Processed Segment 25 / 100\n",
            "Segment 25 size before processing: (4501, 2)\n",
            "Processed Segment 26 / 100\n",
            "Segment 26 size before processing: (4501, 2)\n",
            "Processed Segment 27 / 100\n",
            "Segment 27 size before processing: (4501, 2)\n",
            "Processed Segment 28 / 100\n",
            "Segment 28 size before processing: (4501, 2)\n",
            "Processed Segment 29 / 100\n",
            "Segment 29 size before processing: (4501, 2)\n",
            "Processed Segment 30 / 100\n",
            "Segment 30 size before processing: (4501, 2)\n",
            "Processed Segment 31 / 100\n",
            "Segment 31 size before processing: (4501, 2)\n",
            "Processed Segment 32 / 100\n",
            "Segment 32 size before processing: (4501, 2)\n",
            "Processed Segment 33 / 100\n",
            "Segment 33 size before processing: (4501, 2)\n",
            "Processed Segment 34 / 100\n",
            "Segment 34 size before processing: (4501, 2)\n",
            "Processed Segment 35 / 100\n",
            "Segment 35 size before processing: (4501, 2)\n",
            "Processed Segment 36 / 100\n",
            "Segment 36 size before processing: (4501, 2)\n",
            "Processed Segment 37 / 100\n",
            "Segment 37 size before processing: (4501, 2)\n",
            "Processed Segment 38 / 100\n",
            "Segment 38 size before processing: (4501, 2)\n",
            "Processed Segment 39 / 100\n",
            "Segment 39 size before processing: (4501, 2)\n",
            "Processed Segment 40 / 100\n",
            "Segment 40 size before processing: (4501, 2)\n",
            "Processed Segment 41 / 100\n",
            "Segment 41 size before processing: (4501, 2)\n",
            "Processed Segment 42 / 100\n",
            "Segment 42 size before processing: (4501, 2)\n",
            "Processed Segment 43 / 100\n",
            "Segment 43 size before processing: (4501, 2)\n",
            "Processed Segment 44 / 100\n",
            "Segment 44 size before processing: (4501, 2)\n",
            "Processed Segment 45 / 100\n",
            "Segment 45 size before processing: (4501, 2)\n",
            "Processed Segment 46 / 100\n",
            "Segment 46 size before processing: (4501, 2)\n",
            "Processed Segment 47 / 100\n",
            "Segment 47 size before processing: (4501, 2)\n",
            "Processed Segment 48 / 100\n",
            "Segment 48 size before processing: (4501, 2)\n",
            "Processed Segment 49 / 100\n",
            "Segment 49 size before processing: (4501, 2)\n",
            "Processed Segment 50 / 100\n",
            "Segment 50 size before processing: (4501, 2)\n",
            "Processed Segment 51 / 100\n",
            "Segment 51 size before processing: (4501, 2)\n",
            "Processed Segment 52 / 100\n",
            "Segment 52 size before processing: (4501, 2)\n",
            "Processed Segment 53 / 100\n",
            "Segment 53 size before processing: (4501, 2)\n",
            "Processed Segment 54 / 100\n",
            "Segment 54 size before processing: (4501, 2)\n",
            "Processed Segment 55 / 100\n",
            "Segment 55 size before processing: (4501, 2)\n",
            "Processed Segment 56 / 100\n",
            "Segment 56 size before processing: (4501, 2)\n",
            "Processed Segment 57 / 100\n",
            "Segment 57 size before processing: (4501, 2)\n",
            "Processed Segment 58 / 100\n",
            "Segment 58 size before processing: (4501, 2)\n",
            "Processed Segment 59 / 100\n",
            "Segment 59 size before processing: (4501, 2)\n",
            "Processed Segment 60 / 100\n",
            "Segment 60 size before processing: (4501, 2)\n",
            "Processed Segment 61 / 100\n",
            "Segment 61 size before processing: (4501, 2)\n",
            "Processed Segment 62 / 100\n",
            "Segment 62 size before processing: (4501, 2)\n",
            "Processed Segment 63 / 100\n",
            "Segment 63 size before processing: (4501, 2)\n",
            "Processed Segment 64 / 100\n",
            "Segment 64 size before processing: (4501, 2)\n",
            "Processed Segment 65 / 100\n",
            "Segment 65 size before processing: (4501, 2)\n",
            "Processed Segment 66 / 100\n",
            "Segment 66 size before processing: (4501, 2)\n",
            "Processed Segment 67 / 100\n",
            "Segment 67 size before processing: (4501, 2)\n",
            "Processed Segment 68 / 100\n",
            "Segment 68 size before processing: (4501, 2)\n",
            "Processed Segment 69 / 100\n",
            "Segment 69 size before processing: (4501, 2)\n",
            "Processed Segment 70 / 100\n",
            "Segment 70 size before processing: (4501, 2)\n",
            "Processed Segment 71 / 100\n",
            "Segment 71 size before processing: (4501, 2)\n",
            "Processed Segment 72 / 100\n",
            "Segment 72 size before processing: (4501, 2)\n",
            "Processed Segment 73 / 100\n",
            "Segment 73 size before processing: (4501, 2)\n",
            "Processed Segment 74 / 100\n",
            "Segment 74 size before processing: (4501, 2)\n",
            "Processed Segment 75 / 100\n",
            "Segment 75 size before processing: (4501, 2)\n",
            "Processed Segment 76 / 100\n",
            "Segment 76 size before processing: (4501, 2)\n",
            "Processed Segment 77 / 100\n",
            "Segment 77 size before processing: (4501, 2)\n",
            "Processed Segment 78 / 100\n",
            "Segment 78 size before processing: (4501, 2)\n",
            "Processed Segment 79 / 100\n",
            "Segment 79 size before processing: (4501, 2)\n",
            "Processed Segment 80 / 100\n",
            "Segment 80 size before processing: (4501, 2)\n",
            "Processed Segment 81 / 100\n",
            "Segment 81 size before processing: (4501, 2)\n",
            "Processed Segment 82 / 100\n",
            "Segment 82 size before processing: (4501, 2)\n",
            "Processed Segment 83 / 100\n",
            "Segment 83 size before processing: (4501, 2)\n",
            "Processed Segment 84 / 100\n",
            "Segment 84 size before processing: (4501, 2)\n",
            "Processed Segment 85 / 100\n",
            "Segment 85 size before processing: (4501, 2)\n",
            "Processed Segment 86 / 100\n",
            "Segment 86 size before processing: (4501, 2)\n",
            "Processed Segment 87 / 100\n",
            "Segment 87 size before processing: (4501, 2)\n",
            "Processed Segment 88 / 100\n",
            "Segment 88 size before processing: (4501, 2)\n",
            "Processed Segment 89 / 100\n",
            "Segment 89 size before processing: (4501, 2)\n",
            "Processed Segment 90 / 100\n",
            "Segment 90 size before processing: (4501, 2)\n",
            "Processed Segment 91 / 100\n",
            "Segment 91 size before processing: (4501, 2)\n",
            "Processed Segment 92 / 100\n",
            "Segment 92 size before processing: (4501, 2)\n",
            "Processed Segment 93 / 100\n",
            "Segment 93 size before processing: (4501, 2)\n",
            "Processed Segment 94 / 100\n",
            "Segment 94 size before processing: (4501, 2)\n",
            "Processed Segment 95 / 100\n",
            "Segment 95 size before processing: (4501, 2)\n",
            "Processed Segment 96 / 100\n",
            "Segment 96 size before processing: (4501, 2)\n",
            "Processed Segment 97 / 100\n",
            "Segment 97 size before processing: (4501, 2)\n",
            "Processed Segment 98 / 100\n",
            "Segment 98 size before processing: (4501, 2)\n",
            "Processed Segment 99 / 100\n",
            "Segment 99 size before processing: (4577, 2)\n",
            "Processed Segment 100 / 100\n",
            "Combined DataFrame size before cleaning: (414100, 321)\n",
            "Combined DataFrame size after cleaning: (414100, 321)\n",
            "Final combined DataFrame saved.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>use_of_ip</th>\n",
              "      <th>abnormal_url</th>\n",
              "      <th>google_index</th>\n",
              "      <th>count.</th>\n",
              "      <th>count-www</th>\n",
              "      <th>count@</th>\n",
              "      <th>count_dir</th>\n",
              "      <th>count_embed_domian</th>\n",
              "      <th>short_url</th>\n",
              "      <th>count%</th>\n",
              "      <th>...</th>\n",
              "      <th>vector_294</th>\n",
              "      <th>vector_295</th>\n",
              "      <th>vector_296</th>\n",
              "      <th>vector_297</th>\n",
              "      <th>vector_298</th>\n",
              "      <th>vector_299</th>\n",
              "      <th>vector_300</th>\n",
              "      <th>url</th>\n",
              "      <th>error</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.897669</td>\n",
              "      <td>0.557779</td>\n",
              "      <td>0.763385</td>\n",
              "      <td>0.165734</td>\n",
              "      <td>-0.631518</td>\n",
              "      <td>0.781222</td>\n",
              "      <td>-0.030970</td>\n",
              "      <td>https://www.google.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.678099</td>\n",
              "      <td>0.432011</td>\n",
              "      <td>0.378192</td>\n",
              "      <td>0.383078</td>\n",
              "      <td>-0.593755</td>\n",
              "      <td>0.776778</td>\n",
              "      <td>-0.604513</td>\n",
              "      <td>https://www.youtube.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.510948</td>\n",
              "      <td>0.617345</td>\n",
              "      <td>0.438434</td>\n",
              "      <td>0.170376</td>\n",
              "      <td>-0.708871</td>\n",
              "      <td>0.919022</td>\n",
              "      <td>-0.545725</td>\n",
              "      <td>https://www.facebook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.532566</td>\n",
              "      <td>0.311296</td>\n",
              "      <td>0.644168</td>\n",
              "      <td>0.326366</td>\n",
              "      <td>-0.441958</td>\n",
              "      <td>0.480788</td>\n",
              "      <td>-0.122136</td>\n",
              "      <td>https://www.baidu.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.839042</td>\n",
              "      <td>1.002042</td>\n",
              "      <td>0.684355</td>\n",
              "      <td>0.963773</td>\n",
              "      <td>-0.380508</td>\n",
              "      <td>1.413328</td>\n",
              "      <td>-0.263456</td>\n",
              "      <td>https://www.wikipedia.org</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414095</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.597841</td>\n",
              "      <td>0.170621</td>\n",
              "      <td>0.167211</td>\n",
              "      <td>0.245986</td>\n",
              "      <td>-0.225973</td>\n",
              "      <td>0.633880</td>\n",
              "      <td>-0.213304</td>\n",
              "      <td>https://webmailloutlookupdate.editor.multiscre...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414096</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.854994</td>\n",
              "      <td>0.782215</td>\n",
              "      <td>0.441026</td>\n",
              "      <td>0.160874</td>\n",
              "      <td>-0.429039</td>\n",
              "      <td>0.845653</td>\n",
              "      <td>-0.108863</td>\n",
              "      <td>https://sites.google.com/site/recoeveryhelp2098/</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414097</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.210894</td>\n",
              "      <td>0.235877</td>\n",
              "      <td>-0.015319</td>\n",
              "      <td>0.197519</td>\n",
              "      <td>-0.217140</td>\n",
              "      <td>0.309676</td>\n",
              "      <td>-0.202957</td>\n",
              "      <td>http://kushal308kumar.maskarea.com/fileman/Upl...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414098</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Length of values (4501) does not match length ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414099</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Length of values (4577) does not match length ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>414100 rows × 321 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        use_of_ip  abnormal_url  google_index  count.  count-www  count@  \\\n",
              "0             0.0           1.0           1.0     2.0        1.0     0.0   \n",
              "1             0.0           1.0           1.0     2.0        1.0     0.0   \n",
              "2             0.0           1.0           1.0     2.0        1.0     0.0   \n",
              "3             0.0           1.0           1.0     2.0        1.0     0.0   \n",
              "4             0.0           1.0           1.0     2.0        1.0     0.0   \n",
              "...           ...           ...           ...     ...        ...     ...   \n",
              "414095        0.0           1.0           1.0     3.0        0.0     0.0   \n",
              "414096        0.0           1.0           1.0     2.0        0.0     0.0   \n",
              "414097        0.0           1.0           1.0     2.0        0.0     0.0   \n",
              "414098        NaN           NaN           NaN     NaN        NaN     NaN   \n",
              "414099        NaN           NaN           NaN     NaN        NaN     NaN   \n",
              "\n",
              "        count_dir  count_embed_domian  short_url  count%  ...  vector_294  \\\n",
              "0             0.0                 0.0        0.0     0.0  ...   -0.897669   \n",
              "1             0.0                 0.0        0.0     0.0  ...   -0.678099   \n",
              "2             0.0                 0.0        0.0     0.0  ...   -0.510948   \n",
              "3             0.0                 0.0        0.0     0.0  ...   -0.532566   \n",
              "4             0.0                 0.0        0.0     0.0  ...   -0.839042   \n",
              "...           ...                 ...        ...     ...  ...         ...   \n",
              "414095        2.0                 0.0        0.0     0.0  ...   -0.597841   \n",
              "414096        3.0                 0.0        0.0     0.0  ...   -0.854994   \n",
              "414097        3.0                 0.0        0.0     0.0  ...   -0.210894   \n",
              "414098        NaN                 NaN        NaN     NaN  ...         NaN   \n",
              "414099        NaN                 NaN        NaN     NaN  ...         NaN   \n",
              "\n",
              "        vector_295  vector_296  vector_297  vector_298  vector_299  \\\n",
              "0         0.557779    0.763385    0.165734   -0.631518    0.781222   \n",
              "1         0.432011    0.378192    0.383078   -0.593755    0.776778   \n",
              "2         0.617345    0.438434    0.170376   -0.708871    0.919022   \n",
              "3         0.311296    0.644168    0.326366   -0.441958    0.480788   \n",
              "4         1.002042    0.684355    0.963773   -0.380508    1.413328   \n",
              "...            ...         ...         ...         ...         ...   \n",
              "414095    0.170621    0.167211    0.245986   -0.225973    0.633880   \n",
              "414096    0.782215    0.441026    0.160874   -0.429039    0.845653   \n",
              "414097    0.235877   -0.015319    0.197519   -0.217140    0.309676   \n",
              "414098         NaN         NaN         NaN         NaN         NaN   \n",
              "414099         NaN         NaN         NaN         NaN         NaN   \n",
              "\n",
              "        vector_300                                                url  \\\n",
              "0        -0.030970                             https://www.google.com   \n",
              "1        -0.604513                            https://www.youtube.com   \n",
              "2        -0.545725                           https://www.facebook.com   \n",
              "3        -0.122136                              https://www.baidu.com   \n",
              "4        -0.263456                          https://www.wikipedia.org   \n",
              "...            ...                                                ...   \n",
              "414095   -0.213304  https://webmailloutlookupdate.editor.multiscre...   \n",
              "414096   -0.108863   https://sites.google.com/site/recoeveryhelp2098/   \n",
              "414097   -0.202957  http://kushal308kumar.maskarea.com/fileman/Upl...   \n",
              "414098         NaN                                                NaN   \n",
              "414099         NaN                                                NaN   \n",
              "\n",
              "                                                    error  type  \n",
              "0                                                     NaN     0  \n",
              "1                                                     NaN     0  \n",
              "2                                                     NaN     0  \n",
              "3                                                     NaN     0  \n",
              "4                                                     NaN     0  \n",
              "...                                                   ...   ...  \n",
              "414095                                                NaN     1  \n",
              "414096                                                NaN     1  \n",
              "414097                                                NaN     1  \n",
              "414098  Length of values (4501) does not match length ...     1  \n",
              "414099  Length of values (4577) does not match length ...     1  \n",
              "\n",
              "[414100 rows x 321 columns]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "process_in_segments(base_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
